# -*- coding: utf-8 -*-
"""gradiochat.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16CMmklgBnGxA38Szk-h09D_nhLOw2BN_
"""

!pip install gradio
import gradio as gr
import requests

# Replace with your backend API URL (Flask/FastAPI running your Colab logic)
BACKEND_API_URL = "http://your-backend-api.com/get_recipes"

# Replace with your OpenRouter or Together.ai API details
LLM_API_URL = "https://openrouter.ai/chat"
LLM_API_KEY = "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"
MODEL_NAME = "google/gemini-2.0-flash-exp:free"  # or "llama", "mixtral", etc.

def chatbot_response(user_message, history):
    try:
        # Step 1: Send message to LLM
        llm_payload = {
            "model": MODEL_NAME,
            "messages": [{"role": "user", "content": user_message}]
        }
        llm_headers = {"Authorization": f"Bearer {LLM_API_KEY}"}
        llm_resp = requests.post(LLM_API_URL, json=llm_payload, headers=llm_headers)
        llm_data = llm_resp.json()

        # Extract intent (dish type, flavor, cuisine)
        llm_reply = llm_data['choices'][0]['message']['content']
        # For demo, we'll just echo the user input as dish type
        dish_type = user_message  # Ideally, parse from llm_reply

        # Step 2: Call backend recipe API
        backend_resp = requests.get(BACKEND_API_URL, params={"dish_type": dish_type})
        backend_data = backend_resp.json()

        recipes = backend_data.get('recipes', [])
        if not recipes:
            reply = "Sorry, I couldn't find any matching recipes right now."
        else:
            reply = "Here are 3 recipes I found:\n"
            for i, recipe in enumerate(recipes, 1):
                reply += f"{i}. {recipe['title']} - {recipe['url']}\n"
            reply += "\nWhich one would you like details for?"

    except Exception as e:
        reply = f"⚠️ Error: {str(e)}"

    history.append((user_message, reply))
    return history

gr.ChatInterface(chatbot_response).launch()